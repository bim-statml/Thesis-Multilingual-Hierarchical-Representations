{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled words saved to sampled_words.json\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "random.seed(42)\n",
    "\n",
    "# Read words from the file\n",
    "with open('top-1000-nouns.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "\n",
    "# Check if there are at least 150 words in the file\n",
    "if len(words) < 150:\n",
    "    raise ValueError(\"The file contains fewer than 150 words.\")\n",
    "\n",
    "# Randomly sample 150 words\n",
    "sampled_words = random.sample(words, 100)\n",
    "\n",
    "# Structure the data for JSON\n",
    "data = {\"English\": sampled_words}\n",
    "\n",
    "# Save the sampled words to a JSON file\n",
    "with open('sampled_words.json', 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print('Sampled words saved to sampled_words.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sampled_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from googletrans import Translator\n",
    "t = Translator()\n",
    "\n",
    "def translate_words(words,dest_lang):\n",
    "    translated_words = []\n",
    "    for word in words:\n",
    "        translated_word = t.translate(word, src='en', dest=dest_lang).text\n",
    "        translated_words.append(translated_word)\n",
    "        \n",
    "    return(translated_words)\n",
    "\n",
    "\n",
    "def sample_words(file_name,rand_seed):\n",
    "    random.seed(rand_seed)\n",
    "    with open(file_name, 'r') as f:\n",
    "        words = f.read().splitlines()\n",
    "    sampled_words = random.sample(words, 100)\n",
    "    \n",
    "    return(sampled_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_verbs = sample_words('english_verbs.txt',42)\n",
    "german_verbs = translate_words(sample_words('english_verbs.txt',101),'de')\n",
    "dutch_verbs = translate_words(sample_words('english_verbs.txt',101),'nl')\n",
    "swedish_verbs = translate_words(sample_words('english_verbs.txt',101),'sv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_verbs = sample_words('spanish_verbs_clean.txt',42)\n",
    "french_verbs = translate_words(sample_words('spanish_verbs_clean.txt',101),'fr')\n",
    "portuguese_verbs = translate_words(sample_words('spanish_verbs_clean.txt',333),'pt')\n",
    "italian_verbs = translate_words(sample_words('spanish_verbs_clean.txt',911),'it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accentuer',\n",
       " 'congelé',\n",
       " 'désagréger',\n",
       " 'décréter',\n",
       " 'vestir',\n",
       " 'mordre',\n",
       " 'ruborizarse',\n",
       " 'aménazar',\n",
       " 'appartement',\n",
       " 'temer',\n",
       " 'ducharse',\n",
       " 'arbitre',\n",
       " 'empezar',\n",
       " 'granizar',\n",
       " 'sollozar',\n",
       " 'Arriesgar',\n",
       " 'cerciorarse',\n",
       " 'dormir',\n",
       " 'appeler',\n",
       " 'récupérer',\n",
       " 'aider',\n",
       " 'excuser',\n",
       " 'déséchar',\n",
       " 'cortar',\n",
       " 'beber',\n",
       " 'récidiver',\n",
       " 'soignant',\n",
       " 'ver',\n",
       " 'notifier',\n",
       " 'désirer',\n",
       " 'saler',\n",
       " 'chèque',\n",
       " 'lorgner',\n",
       " 'envoyer',\n",
       " 'Alisar',\n",
       " 'Chupar',\n",
       " 'platique',\n",
       " 'publier',\n",
       " 'distraire',\n",
       " 'arrancar',\n",
       " 'renouer',\n",
       " 'déléguer',\n",
       " 'empêcher',\n",
       " 'doler',\n",
       " 'annuler',\n",
       " 'amueblar',\n",
       " 'comparer',\n",
       " 'ahogar',\n",
       " 'apretar',\n",
       " 'débuter',\n",
       " 'apparenter',\n",
       " 'conspirer',\n",
       " 'fêter',\n",
       " 'perdre',\n",
       " 'décliner',\n",
       " 'olvidar',\n",
       " 'enfermarse',\n",
       " 'quebrar',\n",
       " \"à l'envers\",\n",
       " 'pouvoir',\n",
       " 'bénéficiaire',\n",
       " 'apenar',\n",
       " 'produire',\n",
       " 'aburrir',\n",
       " 'robar',\n",
       " 'durar',\n",
       " 'venger',\n",
       " 'bostezar',\n",
       " 'masticer',\n",
       " 'cincelier',\n",
       " 'accuser',\n",
       " 'observer',\n",
       " 'asesinar',\n",
       " 'Nadar',\n",
       " 'accepter',\n",
       " 'fabriquer',\n",
       " 'abrigar',\n",
       " 'commenter',\n",
       " 'contribuer',\n",
       " 'agotar',\n",
       " 'recevoir',\n",
       " 'dépositaire',\n",
       " 'fumer',\n",
       " 'arpentir',\n",
       " 'calomniateur',\n",
       " 'concurrent',\n",
       " 'consultant',\n",
       " 'archiveur',\n",
       " 'aturdir',\n",
       " 'complaisant',\n",
       " 'besoin',\n",
       " 'analyser',\n",
       " 'abuser',\n",
       " 'seguir',\n",
       " 'couver',\n",
       " 'remplacer',\n",
       " 'tomar',\n",
       " 'insister',\n",
       " 'connecter',\n",
       " 'arrêter']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure the data for JSON\n",
    "data = {\"English\": english_verbs,\n",
    "        \"German\": german_verbs,\n",
    "        \"Dutch\": dutch_verbs,\n",
    "        \"Swedish\": swedish_verbs\n",
    "        }\n",
    "\n",
    "# Save the sampled words to a JSON file\n",
    "with open('Germanic_full_verbs.json', 'w', encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure the data for JSON\n",
    "data = {\"Spanish\": spanish_verbs,\n",
    "        \"French\": french_verbs,\n",
    "        \"Portuguese\": portuguese_verbs,\n",
    "        \"Italian\": italian_verbs\n",
    "        }\n",
    "\n",
    "# Save the sampled words to a JSON file\n",
    "with open('Romance_full_verbs.json', 'w', encoding=\"utf-8\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'spanish_nouns.xlsx'  # Replace 'your_file.xlsx' with the path to your file\n",
    "data = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Extract data from the second column\n",
    "# Adjust the column index or name according to your file structure\n",
    "second_column_data = data.iloc[:, 1]  # '1' assumes columns start at 0\n",
    "\n",
    "second_column_text = second_column_data.to_string(index=False, header=False)\n",
    "\n",
    "# Save the data to a text file\n",
    "\n",
    "with open('output.txt', 'w') as file:\n",
    "    file.write(second_column_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "excel_file = 'spanish_verbs.xlsx'  # Replace with your Excel file path\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# Open a text file to write the output\n",
    "with open('spanish_verbs.txt', 'w') as file:\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Convert each row to a string and write it to the text file\n",
    "        file.write(', '.join(row.astype(str)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your text file\n",
    "file_path = 'output.txt'\n",
    "\n",
    "# Read the first 150 lines from the text file\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = [next(file).strip() for _ in range(150)]\n",
    "\n",
    "# Create a dictionary with these lines under the category \"Spanish\"\n",
    "data = {'German': german_words}\n",
    "\n",
    "# Path to the JSON file where you want to save the data\n",
    "json_file_path = 'Romance.json'\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Read the file\n",
    "with open('spanish-2000.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Process the lines\n",
    "filtered_lines = []\n",
    "for line in lines:\n",
    "    # Remove the text after '/' and the '/' symbol itself\n",
    "    line = re.sub(r'/.*', '', line).strip()\n",
    "    # Keep the line only if it has one or no words\n",
    "    if len(line.split()) <= 1:\n",
    "        filtered_lines.append(line + '\\n')\n",
    "\n",
    "# Write the filtered lines back to a new file\n",
    "with open('filtered_file.txt', 'w') as file:\n",
    "    file.writelines(filtered_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with these lines under the category \"Spanish\"\n",
    "data = {'Spanish': spanish_verbs}\n",
    "\n",
    "# Path to the JSON file where you want to save the data\n",
    "json_file_path = 'Romance_verbs.json'\n",
    "\n",
    "# Write the data to a JSON file\n",
    "with open(json_file_path, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spanish_verbs.txt', 'r') as file, open('spanish_verbs_clean.txt', 'w') as output:\n",
    "    # Read through each line in the file\n",
    "    for line in file:\n",
    "        # Split the line into words and check if it contains more than one word\n",
    "        if len(line.split()) <= 1:\n",
    "            # If it contains 0 or 1 word, write it to the new file\n",
    "            output.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
